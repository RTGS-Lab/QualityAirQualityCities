{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b91a7c3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Observed Air Quality (PurpleAir)\n",
    "\n",
    "This notebook retrieves readings from PurpleAir Sensors in Minneapolis and cleans the entries and saves the results as a csv file.\n",
    "\n",
    "Documentation is available here: https://api.purpleair.com.\n",
    "You can read this article for help getting started: https://community.purpleair.com/t/making-api-calls-with-the-purpleair-api/180.\n",
    "\n",
    "From PurpleAir: \n",
    "\n",
    "\"The data from individual sensors will update no less than every 30 seconds. As a courtesy, we ask that you limit the number of requests to no more than once every 1 to 10 minutes, assuming you are only using the API to obtain data from sensors. If retrieving data from multiple sensors at once, please send a single request rather than individual requests in succession.\n",
    "\n",
    "The PurpleAir historical API is released as of July 18, 2022. For more information, view this post: https://community.purpleair.com/t/new-version-of-the-purpleair-api-on-july-18th/1251.\n",
    "\n",
    "Please let us know if you have any questions or concerns, and have a great day!\"\n",
    "\n",
    "A paper on this process: https://doi.org/10.5194/amt-14-4617-2021 (Link for [Download](https://www.researchgate.net/publication/352663348_Development_and_application_of_a_United_States-wide_correction_for_PM25_data_collected_with_the_PurpleAir_sensor) )\n",
    "\n",
    "Chat on which PM Estimate to use: https://community.purpleair.com/t/pm2-5-algorithms/3972/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f319216",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Packages\n",
    "\n",
    "# File manipulation\n",
    "\n",
    "import os # For working with Operating System\n",
    "import requests # Accessing the Web\n",
    "import datetime as dt # Working with dates/times\n",
    "import io # Input/Output Bytes objects\n",
    "\n",
    "# Analysis\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacecd5a",
   "metadata": {},
   "source": [
    "## Set Working Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d33a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get CWD\n",
    "\n",
    "# cwd = os.getcwd() # This is a global variable for where the notebook is (must change if running in arcpro)\n",
    "\n",
    "# # Create GeoDataBase\n",
    "# # This is the communal GeoDataBase\n",
    "\n",
    "# if not os.path.exists(os.path.join(cwd, '..', '..', 'data', 'QAQC.gdb')): # If it doesn't exist, create it\n",
    "\n",
    "#     arcpy.management.CreateFileGDB(os.path.join(cwd, '..', '..', 'data'), 'QAQC')\n",
    "\n",
    "# # Make it workspace\n",
    "\n",
    "# arcpy.env.workspace = os.path.join(cwd, '..', '..', 'data', 'QAQC.gdb')\n",
    "\n",
    "# arcpy.env.overwriteOutput = True # Overwrite layers is okay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc473325",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b73b4a0-daaf-4a02-b0e4-4a218f2afa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_threshold = 15 # Micgrograms per meter cubed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a91ea8-28a7-4408-a9bf-8ad6b386b231",
   "metadata": {},
   "source": [
    "### Summary Statistics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbe32cea-9e74-4cff-aa14-7427277fb63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n_observations']\n",
      "[<class 'int'>]\n",
      "[<function n_observations at 0x7f7c88dcf370>]\n"
     ]
    }
   ],
   "source": [
    "%run Summary_Functions.py\n",
    "\n",
    "print(summary_stats)\n",
    "print(summary_stats_dtypes)\n",
    "print(summary_stats_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f861d3ec-7915-4a0e-9030-091f85551b8d",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9add3cf8-5631-4836-8729-84baaa29af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QAQC\n",
    "\n",
    "def qaqc(df):\n",
    "    '''This function will perform some basic QAQC\n",
    "    '''\n",
    "    \n",
    "    clean_df = df.copy()\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    \n",
    "    clean_df['timestamp'] = pd.to_datetime(clean_df['timestamp'], unit='s')\n",
    "    \n",
    "    # Remove NaNs\n",
    "    \n",
    "    clean_df = clean_df.dropna()\n",
    "    \n",
    "    return clean_df\n",
    "\n",
    "# Remove and record Spikes\n",
    "\n",
    "def get_spikes(df, spike_threshold):\n",
    "    '''This function removes spikes from a dataframe \n",
    "    and returns both the new dataframe\n",
    "    and a separate spike dataframe\n",
    "    '''\n",
    "    \n",
    "    quantile_95 = np.quantile(df.pm25.values, 0.95)\n",
    "    \n",
    "    condition = (df.pm25 > spike_threshold)\n",
    "    \n",
    "    spikes = df[condition]\n",
    "    \n",
    "    # What if the day had particularly high readings?\n",
    "    \n",
    "    if spike_threshold < quantile_95: # If true, we should include in analysis\n",
    "    \n",
    "        condition = (df.pm25 > quantile_95) # Update the condition to use quantile\n",
    "    \n",
    "    not_spikes = df[~condition]        \n",
    "    \n",
    "    return not_spikes, spikes\n",
    "\n",
    "# Get Summary Stats\n",
    "\n",
    "def get_summary_stats(df):\n",
    "    ''' This is the main function. It will run all of our functions that get summary stats\n",
    "    and return as a list.\n",
    "    '''\n",
    "    \n",
    "    stats = []\n",
    "    \n",
    "    # Run the functions\n",
    "    \n",
    "    for f in summary_stats_functions:\n",
    "        stats += f(df)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae2cb3",
   "metadata": {},
   "source": [
    "### Set Up Parameters for Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dbe69f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your Purple Air api key 51592903-B445-11ED-B6F4-42010A800007\n"
     ]
    }
   ],
   "source": [
    "# This is my personal API key... Please use responsibly!\n",
    "\n",
    "api = input('Please enter your Purple Air api key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa2da2a8-3efa-401e-b204-6e99c3053199",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Query Strings\n",
    "\n",
    "# Average string (in minutes) 1440 is 1 day average\n",
    "\n",
    "avg_string = 'average=10'\n",
    "\n",
    "# Environmental fields\n",
    "\n",
    "env_fields = ['pm2.5_cf_1']\n",
    "\n",
    "env_fields_string = 'fields=' + '%2C%20'.join(env_fields)\n",
    "\n",
    "# My Header\n",
    "\n",
    "my_headers = {'X-API-Key': api}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f0d126",
   "metadata": {},
   "source": [
    "## The Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15516cf4-2085-48db-90ce-43a06fdb7c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Run on  2023-04-19 18:37:00.407138\n"
     ]
    }
   ],
   "source": [
    "## Iterables\n",
    "\n",
    "sensor_ids = [3088, 5582, 11134, 142718, 142720] # This should be an iterable of the sensor ids as integers\n",
    "\n",
    "datelist = pd.date_range(start = dt.datetime(2022,6,15), # June 15, 2022,\n",
    "                         end = dt.datetime.today(),\n",
    "                        normalize = True)\n",
    "\n",
    "print('Last Run on ', dt.datetime.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98951b98-370f-4be1-9f73-e45e6548df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Storage\n",
    "\n",
    "# Daily Summary\n",
    "\n",
    "cols = ['date'] + ['sensor_index'] + summary_stats\n",
    "\n",
    "datatypes = [str, int] + summary_stats_dtypes\n",
    "\n",
    "dtypes = np.dtype(list(zip(cols, datatypes)))\n",
    "\n",
    "daily_summary_df = pd.DataFrame(np.empty(0, dtype = dtypes))\n",
    "\n",
    "# Spikes\n",
    "\n",
    "all_spikes_df = pd.DataFrame(np.empty(0, dtype = [('timestamp', pd._libs.tslibs.timestamps.Timestamp),\n",
    "                                              ('pm25', float),\n",
    "                                              ('sensor_index', int)]\n",
    "                                 )\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdbba671",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 53\u001b[0m\n\u001b[1;32m     49\u001b[0m all_spikes_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([all_spikes_df, spikes], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Get Stats\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m sum_stats \u001b[38;5;241m=\u001b[39m \u001b[43mget_summary_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_no_spikes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Add to the daily summary dataframe\u001b[39;00m\n\u001b[1;32m     57\u001b[0m row \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(datelist[i]\u001b[38;5;241m.\u001b[39mdate()), \u001b[38;5;28mint\u001b[39m(sensor_id)] \u001b[38;5;241m+\u001b[39m sum_stats\n",
      "Cell \u001b[0;32mIn [10], line 56\u001b[0m, in \u001b[0;36mget_summary_stats\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Run the functions\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m summary_stats_functions:\n\u001b[0;32m---> 56\u001b[0m     stats \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m f(df)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stats\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Iterate through the days\n",
    "\n",
    "for i in range(len(datelist)-1):\n",
    "    \n",
    "    # Set up Timestamp for query    \n",
    "    \n",
    "    start_timestamp = int(datelist[i].timestamp())\n",
    "    end_timestamp = int(datelist[i+1].timestamp())\n",
    "    \n",
    "    time_string = 'start_timestamp=' + str(start_timestamp) + '&end_timestamp=' + str(end_timestamp)\n",
    "    \n",
    "    # Iterate through the Sensors\n",
    "    \n",
    "    for sensor_id in sensor_ids:\n",
    "        # Base URL\n",
    "        base_url = f'https://api.purpleair.com/v1/sensors/{sensor_id}/history/csv?'\n",
    "\n",
    "        # Put it all together\n",
    "        query_url = base_url + '&'.join([time_string, avg_string, env_fields_string])\n",
    "\n",
    "        response = requests.get(query_url, headers=my_headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            # Read response as CSV data\n",
    "            csv_data = response.content.decode('utf-8')\n",
    "\n",
    "            if csv_data.count('\\n') == 1: # There is only one line (empty data)\n",
    "                print(f\"No data for sensor {sensor_id} on {datelist[i]}\")\n",
    "                \n",
    "            else:\n",
    "                # Parse CSV data into pandas DataFrame\n",
    "                df_individual_sensor = pd.read_csv(io.StringIO(csv_data),\n",
    "                                                   header=0\n",
    "                                                  )[['time_stamp', 'pm2.5_cf_1']]\n",
    "                \n",
    "                df_individual_sensor.columns = ['timestamp', 'pm25']\n",
    "                \n",
    "                # Perform QAQC\n",
    "                \n",
    "                clean = qaqc(df_individual_sensor)\n",
    "                \n",
    "                # Remove Spikes & Concatenate to main storage of spikes\n",
    "\n",
    "                clean_no_spikes, spikes = get_spikes(clean, spike_threshold)\n",
    "                \n",
    "                spikes['sensor_index'] = int(sensor_id)\n",
    "                \n",
    "                all_spikes_df = pd.concat([all_spikes_df, spikes], ignore_index=True)\n",
    "                \n",
    "                # Get Stats\n",
    "\n",
    "                sum_stats = get_summary_stats(clean_no_spikes)\n",
    "                \n",
    "                # Add to the daily summary dataframe\n",
    "                \n",
    "                row = [str(datelist[i].date()), int(sensor_id)] + sum_stats\n",
    "                \n",
    "                daily_summary_df.loc[len(daily_summary_df.index)] = row\n",
    "        else:\n",
    "            print(f\"Error fetching data for sensor {sensor_id}: {r.status_code} on {datelist[i]}\")\n",
    "            \n",
    "            \n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6678d3b-0846-416f-b0bd-f44b2f8dc0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20feb537-f129-4de3-889f-fcc1e1aa81a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>pm25</th>\n",
       "      <th>sensor_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [timestamp, pm25, sensor_index]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "728fcb69-6c8d-4abb-ac6f-ebc3469481bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save a test dataframe\n",
    "\n",
    "# clean_no_spikes.to_csv('example_df.csv', index = False)\n",
    "\n",
    "# Save it!!!\n",
    "\n",
    "# daily_summary_df.to_csv('daily_summaries.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
